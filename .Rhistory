remaining_weight <- 1
cat("List of available metrics:\n")
for (i in 1:length(metrics)) {
cat(i, ": ", metrics[i], "\n")
}
while (TRUE) {
for (i in 1:length(metrics)) {
selected_metric <- as.integer(readline(paste("Select a metric (", i, " of ", length(metrics), "): ")))
if (is.na(selected_metric) || selected_metric < 1 || selected_metric > length(metrics)) {
cat("Invalid selection.\n")
break
}
cat("Selected metric:", metrics[selected_metric], "\n")
repeat {
weight <- as.numeric(readline(paste("Enter the weight for '", metrics[selected_metric], "' (between 0 and ", remaining_weight, " inclusive): ")))
if (!is.na(weight) && weight >= 0 && weight <= remaining_weight) {
weights[selected_metric] <- weight
remaining_weight <- remaining_weight - weight
break
} else {
cat("The entered weight is invalid or exceeds the remaining weight. Please enter a valid weight.\n")
}
}
if (remaining_weight <= 0) {
break
}
if (i < length(metrics)) {
cat("Remaining weight:", remaining_weight, "\n")
cont <- as.character(readline("Do you want to continue? (y/n): "))
if (cont != "y") {
break
}
}
}
if (sum(weights) == 1) {
cat("Assigned weights:\n")
for (i in 1:length(metrics)) {
cat(metrics[i], " Weight:", weights[i], "\n")
}
break
} else {
cat("Weights must sum up to 1. Please review your values.\n")
remaining_weight <- 1
}
}
for (i in 1:length(metricsResults)) {
key <- as.character(i)
result <- metricsResults[[key]] * weights[i]
results[[key]] <- result
}
results
visit_average
####START USER INTERACTION####
metrics <- c("Metric 1: number of users based on IP field",
"Metric 2: visits per day average",
"Metric 3: failed requests",
"Metric 4: 404 errors",
"Metric 5: Average by day of the week",
"Metric 6: mean time of the session",
"Metric 7: Clicks per session"
)
weights <- numeric(length(metrics))  # Vector to store weights
remaining_weight <- 1
cat("List of available metrics:\n")
for (i in 1:length(metrics)) {
cat(i, ": ", metrics[i], "\n")
}
while (TRUE) {
for (i in 1:length(metrics)) {
selected_metric <- as.integer(readline(paste("Select a metric (", i, " of ", length(metrics), "): ")))
if (is.na(selected_metric) || selected_metric < 1 || selected_metric > length(metrics)) {
cat("Invalid selection.\n")
break
}
cat("Selected metric:", metrics[selected_metric], "\n")
repeat {
weight <- as.numeric(readline(paste("Enter the weight for '", metrics[selected_metric], "' (between 0 and ", remaining_weight, " inclusive): ")))
if (!is.na(weight) && weight >= 0 && weight <= remaining_weight) {
weights[selected_metric] <- weight
remaining_weight <- remaining_weight - weight
break
} else {
cat("The entered weight is invalid or exceeds the remaining weight. Please enter a valid weight.\n")
}
}
if (remaining_weight <= 0) {
break
}
if (i < length(metrics)) {
cat("Remaining weight:", remaining_weight, "\n")
cont <- as.character(readline("Do you want to continue? (y/n): "))
if (cont != "y") {
break
}
}
}
if (sum(weights) == 1) {
cat("Assigned weights:\n")
for (i in 1:length(metrics)) {
cat(metrics[i], " Weight:", weights[i], "\n")
}
break
} else {
cat("Weights must sum up to 1. Please review your values.\n")
remaining_weight <- 1
}
}
results <- list() #weight x metricresult
for (i in 1:length(metricsResults)) {
key <- as.character(i)
result <- metricsResults[[key]] * weights[i]
results[[key]] <- result
}
results
results
resultsSum <- sum(unlist(results))
resultsSum
############ Nueva version
# Load Packages --------------------------------------------------------
if (!require(jsonlite)) {
install.packages("jsonlite")
}
if (!require(ApacheLogProcessor)) {
install.packages("ApacheLogProcessor")
}
if (!require(dplyr)) {
install.packages("dplyr")
}
library(jsonlite)
library(ApacheLogProcessor)
library(dplyr)
# Load Config ------------------------------------------------------
## Config File
#json_path <- file.path("Geo-portal", "config.json")
json_path <- file.path("config.json")
json_content <- readLines(json_path, warn = FALSE)
config <- fromJSON(paste(json_content, collapse = ""))
logPath <- config$logPath
crawlersPath <- config$crawlersPath
log_data <- read.apache.access.log(logPath,num_cores=2)
## Crawlers file
CrawlersPattern_path <- file.path("pattern.txt")
CrawlersPattern <- readLines(CrawlersPattern_path)
Crawlerspattern <- paste(CrawlersPattern, collapse = "|")
log_data <- log_data[!grepl(Crawlerspattern, log_data$useragent, ignore.case = TRUE), ]
# Load data --------------------------------------------------------
input_file <- "/Users/paulaareco/Desktop/ORT/tesis/Geo-portal/Logs/access.log.8"
#leo las lineas del archivo
input_data <- readLines(input_file)
#seria la lista principal, en donde van las ip y las lineas de cada ip
ip_logs <- list()
#regex para encontrar las ip en cada linea (formato ipv4)
ip_pattern <- "\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}"
# Transform log ----------------------------------------------------
#recorro por cada linea
for (line in input_data) {
ip <- regmatches(line, regexpr(ip_pattern, line))
#si encuentro una ip
if (length(ip) > 0) {
#es el primer elemento de ip, porque por linea suponemos que va a haber solo una ip, porque regmatches devuelve una lista de vectores
ip <- ip[[1]]
#me fijo si la ip ya esta en la lista, names devuelve las ip con ese nombre que esten en la lista
if (ip %in% names(ip_logs)) {
#en la posicion de la ip, agrego la linea del log
ip_logs[[ip]] <- c(ip_logs[[ip]], line)
} else {
#sino creo una lista nueva con la ip que no estaba, y agrego esa linea
ip_logs[[ip]] <- list(line)
}
}
}
# Proccess data -------------------------------------------------------------
#### imprime log data #####
output_path <- file.path("TestCases", "outputPlano.csv")
write.csv(log_data, output_path, row.names = FALSE)
df5 = read.apache.access.log(logPath, columns=c("ip", "url", "datetime"))
str(df5)
output_path <- file.path( "TestCases", "outputPlano.csv")
write.csv(df5, output_path, row.names = FALSE)
## Filter Crawlers accounts
### Write log data to a CSV file
output_path <- file.path("TestCases", "outputPlano.csv")
write.csv(log_data, output_path, row.names = FALSE)
## Group by IP and order by data
log_data_ordered <- log_data[order(log_data$ip, log_data$datetime), ]
str(log_data_ordered)
output_path <- file.path("TestCases", "ordered_output.csv")
write.csv(log_data_ordered, output_path , row.names = FALSE)
## Identify Agents
CrawlersPattern_path <- file.path("pattern.txt")
CrawlersPattern <- readLines(CrawlersPattern_path)
Crawlerspattern <- paste(CrawlersPattern, collapse = "|")
log_data <- log_data[!grepl(Crawlerspattern, log_data$useragent, ignore.case = TRUE), ]
### Create a new column for unique identifier (IP + agent)
log_data$unique_id <- paste(log_data$ip, log_data$useragent, sep = "_")
### Identify sessions based on time difference
log_data$datetime <- as.POSIXct(log_data$datetime, format = "%d/%b/%Y:%H:%M:%S", tz = "UTC")
log_data <- log_data %>%
group_by(unique_id) %>%
mutate(time_diff = difftime(datetime, lag(datetime, default = first(datetime)), units = "mins"),
session = cumsum(ifelse(is.na(time_diff) | time_diff > 15, 1, 0))) %>%
ungroup()
### Format session numbers as "session0", "session1", etc.
log_data$session <- paste0("session", log_data$session)
### Print the updated log_data with the session column
print(log_data)
### Write the updated log_data to a CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
write.csv(log_data, output_path, row.names = FALSE)
# Install the necessary packages if not already installed
install.packages(c("dplyr", "lubridate"))
# Load the required libraries
library(dplyr)
library(lubridate)
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
# Convert datetime column to POSIXct format
data$datetime <- as.POSIXct(data$datetime, format = "%Y-%m-%d %H:%M:%S")
# Calculate the time difference between consecutive rows
data$time_diff <- c(0, diff(data$datetime))
# Identify session changes based on IP-Agent tuple and 15-minute threshold
session_change <- c(TRUE, (data$ip[-1] != data$ip[-length(data$ip)]) |
(data$useragent[-1] != data$useragent[-length(data$useragent)]) |
(data$time_diff[-1] > 900))
# Remove missing or incomplete rows
data <- data[complete.cases(data), ]
# Assign session IDs based on session changes
data <- data %>%
mutate(session_id = cumsum(session_change))
# Calculate the mean time of the session
mean_session_time <- data %>%
group_by(session_id) %>%
summarise(mean_time = mean(time_diff))
# Draw a plot of mean session times
plot(mean_session_time$session_id, mean_session_time$mean_time, type = "l",
main = "Mean Session Time", xlab = "Session ID", ylab = "Mean Time (seconds)")
# Calculate the mean time and lines per session
session_summary <- data %>%
group_by(session_id) %>%
summarise(mean_time = mean(time_diff),
lines_per_session = n())
# Calculate the average lines per session
average_lines_per_session <- mean(session_summary$lines_per_session)
# Draw a plot of mean session times
plot(session_summary$session_id, session_summary$mean_time, type = "l",
main = "Mean Session Time", xlab = "Session ID", ylab = "Mean Time (seconds)")
# Print the average lines per session
cat("Average Lines per Session: ", average_lines_per_session, "\n")
#-alterwind log analizer
install.packages(c("dplyr", "lubridate"))
install.packages(c("ggplot2"))
library(ggplot2)
install.packages("stringr") #for metrics using strings
library(stringr)
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
#Metric 1: number of users based on IP field
number_of_users <- length(unique(data$ip))
cat("La cantidad de usuarios distintos es:", number_of_users, "\n")
#Metric 2: visits a day average
data$datetime <- as.Date(data$datetime)
visit_average <- length(unique(data$ip)) / length(unique(data$datetime))
cat("El promedio de visitantes por día es:", visit_average, "\n")
#Metric 3: failed requests
failed_requests <- subset(data, httpcode >= 400)
failed_requests_amount <- nrow(failed_requests)
cat("La cantidad de solicitudes fallidas es:", failed_requests_amount, "\n")
#Metric 4: 404 errors
errors_404 <- subset(data, httpcode == 404)
cat("404 Errors:\n")
cat(errors_404$url, "\n", sep = "\n")
#Metric 5: Average by day of week
data$datetime <- as.Date(data$datetime)
average_by_day <- tapply(data$httpcode, format(data$datetime, "%A"), length)
cat("Average Requests by Day of the Week:\n")
print(average_by_day)
average_by_day <- data.frame(Day = names(average_by_day), AverageRequests = as.numeric(average_by_day))
ggplot(average_by_day, aes(x = Day, y = AverageRequests)) +
geom_bar(stat = "identity", fill = "steelblue") +
xlab("Day of the Week") +
ylab("Average Requests") +
ggtitle("Average Requests per Day of the Week") +
theme_minimal()
#Metric 6: Search engines
data$engine <- str_extract(data$user_agent, "(Google|Bing|Yahoo|DuckDuckGo|Firefox)")
# Install the necessary packages if not already installed
install.packages(c("dplyr", "lubridate"))
# Load the required libraries
library(dplyr)
library(lubridate)
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
# Convert datetime column to POSIXct format
data$datetime <- as.POSIXct(data$datetime, format = "%Y-%m-%d %H:%M:%S")
# Calculate the time difference between consecutive rows
data$time_diff <- c(0, diff(data$datetime))
# Identify session changes based on IP-Agent tuple and 15-minute threshold
session_change <- c(TRUE, (data$ip[-1] != data$ip[-length(data$ip)]) |
(data$useragent[-1] != data$useragent[-length(data$useragent)]) |
(data$time_diff[-1] > 900))
# Remove missing or incomplete rows
data <- data[complete.cases(data), ]
# Assign session IDs based on session changes
data <- data %>%
mutate(session_id = cumsum(session_change))
# Calculate the mean time of the session
mean_session_time <- data %>%
group_by(session_id) %>%
summarise(mean_time = mean(time_diff))
# Draw a plot of mean session times
plot(mean_session_time$session_id, mean_session_time$mean_time, type = "l",
main = "Mean Session Time", xlab = "Session ID", ylab = "Mean Time (seconds)")
# Calculate the mean time and lines per session
session_summary <- data %>%
group_by(session_id) %>%
summarise(mean_time = mean(time_diff),
lines_per_session = n())
# Calculate the average lines per session
average_lines_per_session <- mean(session_summary$lines_per_session)
# Draw a plot of mean session times
plot(session_summary$session_id, session_summary$mean_time, type = "l",
main = "Mean Session Time", xlab = "Session ID", ylab = "Mean Time (seconds)")
# Print the average lines per session
cat("Average Lines per Session: ", average_lines_per_session, "\n")
#-alterwind log analizer
install.packages(c("dplyr", "lubridate"))
install.packages(c("ggplot2"))
library(ggplot2)
install.packages("stringr") #for metrics using strings
library(stringr)
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
#Metric 1: number of users based on IP field
number_of_users <- length(unique(data$ip))
cat("La cantidad de usuarios distintos es:", number_of_users, "\n")
#Metric 2: visits a day average
data$datetime <- as.Date(data$datetime)
visit_average <- length(unique(data$ip)) / length(unique(data$datetime))
cat("El promedio de visitantes por día es:", visit_average, "\n")
#Metric 3: failed requests
failed_requests <- subset(data, httpcode >= 400)
failed_requests_amount <- nrow(failed_requests)
cat("La cantidad de solicitudes fallidas es:", failed_requests_amount, "\n")
#Metric 4: 404 errors
errors_404 <- subset(data, httpcode == 404)
cat("404 Errors:\n")
cat(errors_404$url, "\n", sep = "\n")
#Metric 5: Average by day of week
data$datetime <- as.Date(data$datetime)
average_by_day <- tapply(data$httpcode, format(data$datetime, "%A"), length)
cat("Average Requests by Day of the Week:\n")
print(average_by_day)
average_by_day <- data.frame(Day = names(average_by_day), AverageRequests = as.numeric(average_by_day))
ggplot(average_by_day, aes(x = Day, y = AverageRequests)) +
geom_bar(stat = "identity", fill = "steelblue") +
xlab("Day of the Week") +
ylab("Average Requests") +
ggtitle("Average Requests per Day of the Week") +
theme_minimal()
#Metric 6: Search engines
data$engine <- str_extract(data$user_agent, "(Google|Bing|Yahoo|DuckDuckGo|Firefox)")
####START USER INTERACTION####
metrics <- c("Metric 1: number of users based on IP field",
"Metric 2: visits per day average",
"Metric 3: failed requests",
"Metric 4: 404 errors",
"Metric 5: Average by day of the week",
"Metric 6: mean time of the session",
"Metric 7: Clicks per session"
)
weights <- numeric(length(metrics))  # Vector to store weights
remaining_weight <- 1
cat("List of available metrics:\n")
for (i in 1:length(metrics)) {
cat(i, ": ", metrics[i], "\n")
}
while (TRUE) {
for (i in 1:length(metrics)) {
selected_metric <- as.integer(readline(paste("Select a metric (", i, " of ", length(metrics), "): ")))
if (is.na(selected_metric) || selected_metric < 1 || selected_metric > length(metrics)) {
cat("Invalid selection.\n")
break
}
cat("Selected metric:", metrics[selected_metric], "\n")
repeat {
weight <- as.numeric(readline(paste("Enter the weight for '", metrics[selected_metric], "' (between 0 and ", remaining_weight, " inclusive): ")))
if (!is.na(weight) && weight >= 0 && weight <= remaining_weight) {
weights[selected_metric] <- weight
remaining_weight <- remaining_weight - weight
break
} else {
cat("The entered weight is invalid or exceeds the remaining weight. Please enter a valid weight.\n")
}
}
if (remaining_weight <= 0) {
break
}
if (i < length(metrics)) {
cat("Remaining weight:", remaining_weight, "\n")
cont <- as.character(readline("Do you want to continue? (y/n): "))
if (cont != "y") {
break
}
}
}
if (sum(weights) == 1) {
cat("Assigned weights:\n")
for (i in 1:length(metrics)) {
cat(metrics[i], " Weight:", weights[i], "\n")
}
break
} else {
cat("Weights must sum up to 1. Please review your values.\n")
remaining_weight <- 1
}
}
############ Nueva version
# Load Packages --------------------------------------------------------
if (!require(jsonlite)) {
install.packages("jsonlite")
}
if (!require(ApacheLogProcessor)) {
install.packages("ApacheLogProcessor")
}
if (!require(dplyr)) {
install.packages("dplyr")
}
library(jsonlite)
library(ApacheLogProcessor)
library(dplyr)
# Load Config ------------------------------------------------------
## Config File
#json_path <- file.path("Geo-portal", "config.json")
json_path <- file.path("config.json")
json_content <- readLines(json_path, warn = FALSE)
config <- fromJSON(paste(json_content, collapse = ""))
logPath <- config$logPath
crawlersPath <- config$crawlersPath
log_data <- read.apache.access.log(logPath,num_cores=2)
## Crawlers file
CrawlersPattern_path <- file.path("pattern.txt")
CrawlersPattern <- readLines(CrawlersPattern_path)
Crawlerspattern <- paste(CrawlersPattern, collapse = "|")
log_data <- log_data[!grepl(Crawlerspattern, log_data$useragent, ignore.case = TRUE), ]
# Load data --------------------------------------------------------
input_file <- "/Users/paulaareco/Desktop/ORT/tesis/Geo-portal/Logs/access.log.8"
#leo las lineas del archivo
input_data <- readLines(input_file)
#seria la lista principal, en donde van las ip y las lineas de cada ip
ip_logs <- list()
#regex para encontrar las ip en cada linea (formato ipv4)
ip_pattern <- "\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}"
# Transform log ----------------------------------------------------
#recorro por cada linea
for (line in input_data) {
ip <- regmatches(line, regexpr(ip_pattern, line))
#si encuentro una ip
if (length(ip) > 0) {
#es el primer elemento de ip, porque por linea suponemos que va a haber solo una ip, porque regmatches devuelve una lista de vectores
ip <- ip[[1]]
#me fijo si la ip ya esta en la lista, names devuelve las ip con ese nombre que esten en la lista
if (ip %in% names(ip_logs)) {
#en la posicion de la ip, agrego la linea del log
ip_logs[[ip]] <- c(ip_logs[[ip]], line)
} else {
#sino creo una lista nueva con la ip que no estaba, y agrego esa linea
ip_logs[[ip]] <- list(line)
}
}
}
# Proccess data -------------------------------------------------------------
#### imprime log data #####
output_path <- file.path("TestCases", "outputPlano.csv")
write.csv(log_data, output_path, row.names = FALSE)
df5 = read.apache.access.log(logPath, columns=c("ip", "url", "datetime"))
str(df5)
output_path <- file.path( "TestCases", "outputPlano.csv")
write.csv(df5, output_path, row.names = FALSE)
## Filter Crawlers accounts
### Write log data to a CSV file
output_path <- file.path("TestCases", "outputPlano.csv")
write.csv(log_data, output_path, row.names = FALSE)
## Group by IP and order by data
log_data_ordered <- log_data[order(log_data$ip, log_data$datetime), ]
str(log_data_ordered)
output_path <- file.path("TestCases", "ordered_output.csv")
write.csv(log_data_ordered, output_path , row.names = FALSE)
## Identify Agents
CrawlersPattern_path <- file.path("pattern.txt")
CrawlersPattern <- readLines(CrawlersPattern_path)
Crawlerspattern <- paste(CrawlersPattern, collapse = "|")
log_data <- log_data[!grepl(Crawlerspattern, log_data$useragent, ignore.case = TRUE), ]
### Create a new column for unique identifier (IP + agent)
log_data$unique_id <- paste(log_data$ip, log_data$useragent, sep = "_")
### Identify sessions based on time difference
log_data$datetime <- as.POSIXct(log_data$datetime, format = "%d/%b/%Y:%H:%M:%S", tz = "UTC")
log_data <- log_data %>%
group_by(unique_id) %>%
mutate(time_diff = difftime(datetime, lag(datetime, default = first(datetime)), units = "mins"),
session = cumsum(ifelse(is.na(time_diff) | time_diff > 15, 1, 0))) %>%
ungroup()
### Format session numbers as "session0", "session1", etc.
log_data$session <- paste0("session", log_data$session)
### Print the updated log_data with the session column
print(log_data)
### Write the updated log_data to a CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
write.csv(log_data, output_path, row.names = FALSE)
