install.packages(c("ggplot2"))
library(ggplot2)
install.packages("stringr") #for metrics using strings
library(stringr)
library(lubridate)
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
install.packages(c("ggplot2"))
install.packages(c("ggplot2"))
install.packages(c("ggplot2"))
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
# Metric: The total of unique pages viewed on your website.
total_unique_urls <- length(unique(data$url))
cat("Total of unique pages viewed: ",total_unique_urls)
############ Nueva version
# Load Packages --------------------------------------------------------
if (!require(jsonlite)) {
install.packages("jsonlite")
}
if (!require(ApacheLogProcessor)) {
install.packages("ApacheLogProcessor")
}
if (!require(dplyr)) {
install.packages("dplyr")
}
library(jsonlite)
library(ApacheLogProcessor)
library(dplyr)
# Load Config ------------------------------------------------------
## Config File
#json_path <- file.path("Geo-portal", "config.json")
json_path <- file.path("config.json")
json_content <- readLines(json_path, warn = FALSE)
config <- fromJSON(paste(json_content, collapse = ""))
logPath <- config$logPath
crawlersPath <- config$crawlersPath
log_data <- read.apache.access.log(logPath,num_cores=2)
## Crawlers file
CrawlersPattern_path <- file.path("pattern.txt")
CrawlersPattern <- readLines(CrawlersPattern_path)
Crawlerspattern <- paste(CrawlersPattern, collapse = "|")
log_data <- log_data[!grepl(Crawlerspattern, log_data$useragent, ignore.case = TRUE), ]
# Load data --------------------------------------------------------
input_file <- "/Users/paulaareco/Desktop/ORT/tesis/Geo-portal/Logs/access.log.8"
#leo las lineas del archivo
input_data <- readLines(input_file)
#seria la lista principal, en donde van las ip y las lineas de cada ip
ip_logs <- list()
#regex para encontrar las ip en cada linea (formato ipv4)
ip_pattern <- "\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}"
# Transform log ----------------------------------------------------
#recorro por cada linea
for (line in input_data) {
ip <- regmatches(line, regexpr(ip_pattern, line))
#si encuentro una ip
if (length(ip) > 0) {
#es el primer elemento de ip, porque por linea suponemos que va a haber solo una ip, porque regmatches devuelve una lista de vectores
ip <- ip[[1]]
#me fijo si la ip ya esta en la lista, names devuelve las ip con ese nombre que esten en la lista
if (ip %in% names(ip_logs)) {
#en la posicion de la ip, agrego la linea del log
ip_logs[[ip]] <- c(ip_logs[[ip]], line)
} else {
#sino creo una lista nueva con la ip que no estaba, y agrego esa linea
ip_logs[[ip]] <- list(line)
}
}
}
# Proccess data -------------------------------------------------------------
#### imprime log data #####
output_path <- file.path("TestCases", "outputPlano.csv")
write.csv(log_data, output_path, row.names = FALSE)
df5 = read.apache.access.log(logPath, columns=c("ip", "url", "datetime"))
str(df5)
output_path <- file.path( "TestCases", "outputPlano.csv")
write.csv(df5, output_path, row.names = FALSE)
## Filter Crawlers accounts
### Write log data to a CSV file
output_path <- file.path("TestCases", "outputPlano.csv")
write.csv(log_data, output_path, row.names = FALSE)
## Group by IP and order by data
log_data_ordered <- log_data[order(log_data$ip, log_data$datetime), ]
str(log_data_ordered)
output_path <- file.path("TestCases", "ordered_output.csv")
write.csv(log_data_ordered, output_path , row.names = FALSE)
## Identify Agents
CrawlersPattern_path <- file.path("pattern.txt")
CrawlersPattern <- readLines(CrawlersPattern_path)
Crawlerspattern <- paste(CrawlersPattern, collapse = "|")
log_data <- log_data[!grepl(Crawlerspattern, log_data$useragent, ignore.case = TRUE), ]
### Create a new column for unique identifier (IP + agent)
log_data$unique_id <- paste(log_data$ip, log_data$useragent, sep = "_")
### Identify sessions based on time difference
log_data$datetime <- as.POSIXct(log_data$datetime, format = "%d/%b/%Y:%H:%M:%S", tz = "UTC")
log_data <- log_data %>%
group_by(unique_id) %>%
mutate(time_diff = difftime(datetime, lag(datetime, default = first(datetime)), units = "mins"),
session = cumsum(ifelse(is.na(time_diff) | time_diff > 15, 1, 0))) %>%
ungroup()
### Format session numbers as "session0", "session1", etc.
log_data$session <- paste0("session", log_data$session)
### Print the updated log_data with the session column
print(log_data)
### Write the updated log_data to a CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
write.csv(log_data, output_path, row.names = FALSE)
# Install the necessary packages if not already installed
install.packages(c("dplyr", "lubridate"))
# Load the required libraries
library(dplyr)
library(lubridate)
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
# Convert datetime column to POSIXct format
data$datetime <- as.POSIXct(data$datetime, format = "%Y-%m-%d %H:%M:%S")
# Calculate the time difference between consecutive rows
data$time_diff <- c(0, diff(data$datetime))
# Identify session changes based on IP-Agent tuple and 15-minute threshold
session_change <- c(TRUE, (data$ip[-1] != data$ip[-length(data$ip)]) |
(data$useragent[-1] != data$useragent[-length(data$useragent)]) |
(data$time_diff[-1] > 900))
# Remove missing or incomplete rows
data <- data[complete.cases(data), ]
# Assign session IDs based on session changes
data <- data %>%
mutate(session_id = cumsum(session_change))
# Calculate the mean time of the session
mean_session_time <- data %>%
group_by(session_id) %>%
summarise(mean_time = mean(time_diff))
# Draw a plot of mean session times
plot(mean_session_time$session_id, mean_session_time$mean_time, type = "l",
main = "Mean Session Time", xlab = "Session ID", ylab = "Mean Time (seconds)")
# Metric 13: average time spent (mean time among all visitors)
overall_mean_time <- mean(mean_session_time$mean_time) # Calculate the overall mean session time
cat("Overall mean time: ", overall_mean_time, "\n")
# Calculate the mean time and lines per session
session_summary <- data %>%
group_by(session_id) %>%
summarise(mean_time = mean(time_diff),
lines_per_session = n())
# Calculate the average lines per session
average_lines_per_session <- mean(session_summary$lines_per_session)
# Draw a plot of mean session times
plot(session_summary$session_id, session_summary$mean_time, type = "l",
main = "Mean Session Time", xlab = "Session ID", ylab = "Mean Time (seconds)")
# Print the average lines per session
cat("Average Lines per Session: ", average_lines_per_session, "\n")
install.packages(c("dplyr", "lubridate"))
install.packages(c("ggplot2"))
library(ggplot2)
install.packages("stringr") #for metrics using strings
library(stringr)
library(lubridate)
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
#Metric 1: number of users based on IP field
number_of_users <- length(unique(data$ip))
cat("La cantidad de usuarios distintos es:", number_of_users, "\n")
#Metric 2: visits a day average
data$datetime <- as.Date(data$datetime)
visit_average <- length(unique(data$ip)) / length(unique(data$datetime))
cat("El promedio de visitantes por día es:", visit_average, "\n")
#Metric 3: failed requests
failed_requests <- subset(data, httpcode >= 400)
failed_requests_amount <- nrow(failed_requests)
cat("La cantidad de solicitudes fallidas es:", failed_requests_amount, "\n")
# Percentage of successful requests (status code < 400)
successful_requests <- subset(data, httpcode < 400)
successful_requests_amount <- nrow(successful_requests)
total_requests <- nrow(data) #total number of requests
percentage_successful_requests <- (successful_requests_amount / total_requests) * 100 # % of successful requests
cat("Percentage of successful requests is:", percentage_successful_requests, "%\n")
#Metric 4: 404 errors
errors_404 <- subset(data, httpcode == 404)
cat("404 Errors:\n")
cat(errors_404$url, "\n", sep = "\n")
error_404_amount <- length(errors_404)
cat("Cantidad de errores 404: ", error_404_amount, "\n")
total_requests <- nrow(data) #total number of requests
percentage_non404_requests <- ((total_requests - error_404_amount) / total_requests) * 100
cat("Percentage of non 404 error requests: ", percentage_non404_requests, "\n")
#Metric 5: Average by day of week
data$datetime <- as.Date(data$datetime)
average_by_day <- tapply(data$httpcode, format(data$datetime, "%A"), length)
cat("Average Requests by Day of the Week:\n")
print(average_by_day)
average_by_day <- data.frame(Day = names(average_by_day), AverageRequests = as.numeric(average_by_day))
ggplot(average_by_day, aes(x = Day, y = AverageRequests)) +
geom_bar(stat = "identity", fill = "steelblue") +
xlab("Day of the Week") +
ylab("Average Requests") +
ggtitle("Average Requests per Day of the Week") +
theme_minimal()
#Metric 6: Search engines
search_engines <- c("Google", "Bing", "Yahoo", "DuckDuckGo", "Firefox")
data$engine <- "Other" #initialization
for (engine in search_engines) {
data$engine[str_detect(tolower(data$useragent), tolower(engine))] <- engine
}
engine_counts <- table(data$engine)
print(engine_counts)
# Metric 7: The total number of pages viewed on your website.
total_pages_viewed <- nrow(data)
cat("Total pages viewed: ",total_pages_viewed)
# Metric: The total of unique pages viewed on your website.
total_unique_urls <- length(unique(data$url))
cat("Total of unique pages viewed: ",total_unique_urls)
install.packages(c("ggplot2"))
library(ggplot2)
install.packages("stringr") #for metrics using strings
library(stringr)
library(lubridate)
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
#Metric 1: number of users based on IP field
number_of_users <- length(unique(data$ip))
cat("La cantidad de usuarios distintos es:", number_of_users, "\n")
#Metric 2: visits a day average
data$datetime <- as.Date(data$datetime)
visit_average <- length(unique(data$ip)) / length(unique(data$datetime))
cat("El promedio de visitantes por día es:", visit_average, "\n")
#Metric 3: failed requests
failed_requests <- subset(data, httpcode >= 400)
failed_requests_amount <- nrow(failed_requests)
cat("La cantidad de solicitudes fallidas es:", failed_requests_amount, "\n")
# Percentage of successful requests (status code < 400)
successful_requests <- subset(data, httpcode < 400)
successful_requests_amount <- nrow(successful_requests)
total_requests <- nrow(data) #total number of requests
percentage_successful_requests <- (successful_requests_amount / total_requests) * 100 # % of successful requests
cat("Percentage of successful requests is:", percentage_successful_requests, "%\n")
#Metric 4: 404 errors
errors_404 <- subset(data, httpcode == 404)
cat("404 Errors:\n")
cat(errors_404$url, "\n", sep = "\n")
error_404_amount <- length(errors_404)
cat("Cantidad de errores 404: ", error_404_amount, "\n")
total_requests <- nrow(data) #total number of requests
percentage_non404_requests <- ((total_requests - error_404_amount) / total_requests) * 100
cat("Percentage of non 404 error requests: ", percentage_non404_requests, "\n")
#Metric 5: Average by day of week
data$datetime <- as.Date(data$datetime)
average_by_day <- tapply(data$httpcode, format(data$datetime, "%A"), length)
cat("Average Requests by Day of the Week:\n")
print(average_by_day)
average_by_day <- data.frame(Day = names(average_by_day), AverageRequests = as.numeric(average_by_day))
ggplot(average_by_day, aes(x = Day, y = AverageRequests)) +
geom_bar(stat = "identity", fill = "steelblue") +
xlab("Day of the Week") +
ylab("Average Requests") +
ggtitle("Average Requests per Day of the Week") +
theme_minimal()
#Metric 6: Search engines
search_engines <- c("Google", "Bing", "Yahoo", "DuckDuckGo", "Firefox")
data$engine <- "Other" #initialization
for (engine in search_engines) {
data$engine[str_detect(tolower(data$useragent), tolower(engine))] <- engine
}
engine_counts <- table(data$engine)
print(engine_counts)
# Metric 7: The total number of pages viewed on your website.
total_pages_viewed <- nrow(data)
cat("Total pages viewed: ",total_pages_viewed)
# Metric: The total of unique pages viewed on your website.
total_unique_urls <- length(unique(data$url))
cat("Total of unique pages viewed: ",total_unique_urls)
install.packages(c("ggplot2"))
install.packages(c("ggplot2"))
install.packages(c("ggplot2"))
library(ggplot2)
install.packages("stringr") #for metrics using strings
library(stringr)
library(lubridate)
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
#Metric 1: number of users based on IP field
number_of_users <- length(unique(data$ip))
cat("La cantidad de usuarios distintos es:", number_of_users, "\n")
#Metric 2: visits a day average
data$datetime <- as.Date(data$datetime)
visit_average <- length(unique(data$ip)) / length(unique(data$datetime))
cat("El promedio de visitantes por día es:", visit_average, "\n")
#Metric 3: failed requests
failed_requests <- subset(data, httpcode >= 400)
failed_requests_amount <- nrow(failed_requests)
cat("La cantidad de solicitudes fallidas es:", failed_requests_amount, "\n")
# Percentage of successful requests (status code < 400)
successful_requests <- subset(data, httpcode < 400)
successful_requests_amount <- nrow(successful_requests)
total_requests <- nrow(data) #total number of requests
percentage_successful_requests <- (successful_requests_amount / total_requests) * 100 # % of successful requests
cat("Percentage of successful requests is:", percentage_successful_requests, "%\n")
#Metric 4: 404 errors
errors_404 <- subset(data, httpcode == 404)
cat("404 Errors:\n")
cat(errors_404$url, "\n", sep = "\n")
error_404_amount <- length(errors_404)
cat("Cantidad de errores 404: ", error_404_amount, "\n")
total_requests <- nrow(data) #total number of requests
percentage_non404_requests <- ((total_requests - error_404_amount) / total_requests) * 100
cat("Percentage of non 404 error requests: ", percentage_non404_requests, "\n")
#Metric 5: Average by day of week
data$datetime <- as.Date(data$datetime)
average_by_day <- tapply(data$httpcode, format(data$datetime, "%A"), length)
cat("Average Requests by Day of the Week:\n")
print(average_by_day)
average_by_day <- data.frame(Day = names(average_by_day), AverageRequests = as.numeric(average_by_day))
ggplot(average_by_day, aes(x = Day, y = AverageRequests)) +
geom_bar(stat = "identity", fill = "steelblue") +
xlab("Day of the Week") +
ylab("Average Requests") +
ggtitle("Average Requests per Day of the Week") +
theme_minimal()
#Metric 6: Search engines
search_engines <- c("Google", "Bing", "Yahoo", "DuckDuckGo", "Firefox")
data$engine <- "Other" #initialization
for (engine in search_engines) {
data$engine[str_detect(tolower(data$useragent), tolower(engine))] <- engine
}
engine_counts <- table(data$engine)
print(engine_counts)
# Metric 7: The total number of pages viewed on your website.
total_pages_viewed <- nrow(data)
cat("Total pages viewed: ",total_pages_viewed)
# Metric: The total of unique pages viewed on your website.
total_unique_urls <- length(unique(data$url))
cat("Total of unique pages viewed: ",total_unique_urls)
install.packages(c("ggplot2"))
install.packages(c("ggplot2"))
install.packages(c("ggplot2"))
library(ggplot2)
install.packages("stringr") #for metrics using strings
library(stringr)
library(lubridate)
install.packages(c("ggplot2"))
install.packages(c("ggplot2"))
install.packages(c("ggplot2"))
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
# Metric: The total of unique pages viewed on your website.
total_unique_urls <- length(unique(data$url))
cat("Total of unique pages viewed: ",total_unique_urls)
# Metric 7: The total number of pages viewed on your website.
total_pages_viewed <- nrow(data)
cat("Total pages viewed: ",total_pages_viewed)
#Metric 6: Search engines
search_engines <- c("Google", "Bing", "Yahoo", "DuckDuckGo", "Firefox")
data$engine <- "Other" #initialization
for (engine in search_engines) {
data$engine[str_detect(tolower(data$useragent), tolower(engine))] <- engine
}
install.packages("stringr") #for metrics using strings
library(stringr)
for (engine in search_engines) {
data$engine[str_detect(tolower(data$useragent), tolower(engine))] <- engine
}
engine_counts <- table(data$engine)
print(engine_counts)
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
total_sessions <- nrow(data)
single_page_sessions <- sum(data$size == 1)
bounce_rate <- (single_page_sessions / total_sessions) * 100
cat("El Bounce Rate es:", bounce_rate, "%\n")
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
# Metric: Bounce Rate: The percentage of single-page sessions (users who leave your site after viewing only one page).
total_sessions <- nrow(data)
single_page_sessions <- sum(data$size == 1)
bounce_rate <- (single_page_sessions / total_sessions) * 100
cat("Bounce Rate is:", bounce_rate, "%\n")
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
# Metric: Bounce Rate: The percentage of single-page sessions (users who leave your site after viewing only one page).
total_sessions <- nrow(data)
single_page_sessions <- sum(data$size == 1)
bounce_rate <- (single_page_sessions / total_sessions) * 100
cat("Bounce Rate is:", bounce_rate, "%\n")
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
# Metric: Bounce Rate: The percentage of single-page sessions (users who leave your site after viewing only one page).
unique_page_views <- length(unique(df$url))
# Metric: Bounce Rate: The percentage of single-page sessions (users who leave your site after viewing only one page).
unique_sessions <- length(unique(df$session))
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
# Metric: Bounce Rate: The percentage of single-page sessions (users who leave your site after viewing only one page).
unique_sessions <- length(unique(df$session))
str(df)
df <- read.csv("output_with_session.csv", header = TRUE)
bounce_rate <- with(data, sum(table(session) == 1) / length(unique(session)) * 100)
cat("Bounce Rate:", format(bounce_rate, digits = 2), "%\n")
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
# Metric: Bounce Rate: The percentage of single-page sessions (users who leave your site after viewing only one page).
bounce_rate <- with(data, sum(table(session) == 1) / length(unique(session)) * 100)
cat("Bounce Rate:", format(bounce_rate, digits = 2), "%\n")
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
# Metric: Bounce Rate: The percentage of single-page sessions (users who leave your site after viewing only one page).
bounce_rate <- with(data, sum(table(session) == 1) / length(unique(session)) * 100)
cat("Bounce Rate:", format(bounce_rate, digits = 2), "%\n")
#Metric: Average Amount of pages visited per user
average_pages_per_user <- mean(table(data$session))
print(paste("Average Amount of pages visited per user: ", average_pages_per_user))
#Metric: Average Amount of pages visited per user
average_pages_per_user <- mean(table(data$session))
print(paste("Average Amount of pages visited per user: ", sprintf("%.2f", average_pages_per_user)))
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
#Metric: Average Amount of pages visited per user
average_pages_per_user <- mean(table(data$session))
print(paste("Average Amount of pages visited per user: ", sprintf("%.2f", average_pages_per_user)))
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
#Metric: Average Amount of pages visited per user
average_pages_per_user <- mean(table(data$session))
print(paste("Average Amount of pages visited per user: ", sprintf("%.2f", average_pages_per_user)))
#Metric: Average Amount of pages visited per user
average_pages_per_user <- length(unique(data$url)) / length(unique(data$session))
print(paste("Average Amount of pages visited per user: ", sprintf("%.2f", average_pages_per_user)))
#Metric: Recurring visitors: amount of visitors that came back
recurring_visitors <- length(unique(data$ip)) - length(unique(data$session))
print(paste("Recurring Visitors:", recurring_visitors))
#Metric: Individual Resource Loading Times (average): to help identify elements that slow down page performance.
average_loading_time <- mean(data$time_diff)
print(paste("Individual Resource Loading Times (average):", average_loading_time, "seconds"))
#Metric: Individual Resource Loading Times (average): to help identify elements that slow down page performance.
average_loading_time <- mean(data$time_diff)
print(paste("Individual Resource Loading Times (average):", format(average_loading_time, digits=2)))
#Metric: Individual Resource Loading Times (average): to help identify elements that slow down page performance.
average_loading_time <- mean(data$time_diff)
print(paste("Individual Resource Loading Times (average):", sprintf("%.2f", average_loading_time)))
#Metric 15: average time spent per visitor
average_time_per_visitor <- mean(data$time_diff, na.rm = TRUE)
print(paste("Average Time Spent per Visitor:", sprintf("%.2f", average_time_per_visitor), "seconds"))
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
#Metric 15: average time spent per visitor
average_time_per_visitor <- mean(data$time_diff, na.rm = TRUE)
print(paste("Average Time Spent per Visitor:", sprintf("%.2f", average_time_per_visitor), "seconds"))
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
#Metric 15: average time spent per visitor
average_time_per_visitor <- mean(data$time_diff, na.rm = TRUE)
print(paste("Average Time Spent per Visitor:", sprintf("%.2f", average_time_per_visitor), "seconds"))
#Metric 15: average time spent per visitor
average_time_per_visitor <- mean(data$time_diff, na.rm = TRUE)
print(paste("Average Time Spent per Visitor:", sprintf("%.2f", average_time_per_visitor), "seconds"))
# Metric: Individual Resource Loading Times (average)
average_loading_time <- mean(data$time_diff)
print(paste("Individual Resource Loading Times (average):", sprintf("%.2f", average_loading_time)))
# Metric: Average Time Spent per Visitor
total_time_per_visitor <- sum(data$time_diff, na.rm = TRUE)
average_time_per_visitor <- total_time_per_visitor / unique_sessions
unique_sessions <- length(unique(data$session))
total_time_on_site <- sum(data$time_diff, na.rm = TRUE)
average_time_per_visitor <- total_time_on_site / unique_sessions
print(paste("Average Time Spent per Visitor:", sprintf("%.2f", average_time_per_visitor), "seconds"))
unique_sessions <- length(unique(data$session))
average_time_per_visitor <- total_time_on_site / unique_sessions
print(paste("Average Time Spent per Visitor:", sprintf("%.2f", average_time_per_visitor), "seconds"))
# Metric: Direct Traffic: Amount of users who typed your website URL directly into their browser.
direct_traffic <- sum(is.na(data$referer))
print(paste("Direct Traffic:", direct_traffic, "users"))
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
# Metric: Direct Traffic: Amount of users who typed your website URL directly into their browser.
direct_traffic <- sum(is.na(data$referer))
print(paste("Direct Traffic:", direct_traffic, "users"))
# Metric: Direct Traffic: Amount of users who typed your website URL directly into their browser.
direct_traffic <- sum(is.na(data$referer))
print(paste("Direct Traffic:", direct_traffic, "users"))
# Read the CSV file
output_path <- file.path("TestCases", "output_with_session.csv")
data <- read.csv(output_path)
# Metric: Direct Traffic: Amount of users who typed your website URL directly into their browser.
direct_traffic <- sum(is.na(data$referer))
print(paste("Direct Traffic:", direct_traffic, "users"))
# Metric: Direct Traffic
direct_traffic <- sum(grepl("^\\s*-$", data$referer, perl = TRUE, ignore.case = TRUE))
print(paste("Direct Traffic:", direct_traffic, "users"))
# Metric: Average Time Spent per Page
average_time_per_page <- aggregate(data$time_diff, by = list(data$url), FUN = mean, na.rm = TRUE)
colnames(average_time_per_page) <- c("url", "average_time")
print(average_time_per_page)
# Metric: Average Time Spent per Page
average_time_per_page <- mean(data$time_diff, na.rm = TRUE)
print(paste("Average Time Spent per Page:", sprintf("%.2f", average_time_per_page), "seconds"))
#Metric 15: average time spent per visitor
average_time_per_visitor <- mean(data$time_diff, na.rm = TRUE)
print(paste("Average Time Spent per Visitor:", sprintf("%.2f", average_time_per_visitor), "seconds"))
